{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte-Carlo Dropout for uncertainty prediction on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement Monte-Carlo dropout for uncertainty prediction according to https://arxiv.org/pdf/1506.02142.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we train a model on MNIST using dropout and then at inference time we keep using dropout to estimate the uncertainty of the network by performing several forward passes through the network for each sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "mc_samples = 10 # number of samples for prediciton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "img_rows, img_cols = x_train.shape[1], x_train.shape[2]\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \"\"\"\n",
    "    Use only minimalistic model to get some statistics for misclassifications\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # This dropout layer stays active during testing phase\n",
    "    model.add(Lambda(lambda x: K.dropout(x, level=0.5)))\n",
    "    \n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                optimizer=keras.optimizers.Adadelta(),\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 5s 87us/step - loss: 0.3705 - acc: 0.8928 - val_loss: 0.2005 - val_acc: 0.9402\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1745 - acc: 0.9491 - val_loss: 0.1378 - val_acc: 0.9616\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1375 - acc: 0.9590 - val_loss: 0.1150 - val_acc: 0.9671\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1192 - acc: 0.9647 - val_loss: 0.1046 - val_acc: 0.9666\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1079 - acc: 0.9681 - val_loss: 0.0974 - val_acc: 0.9699\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.0989 - acc: 0.9708 - val_loss: 0.0923 - val_acc: 0.9718\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0934 - acc: 0.9726 - val_loss: 0.0942 - val_acc: 0.9722\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.0859 - acc: 0.9744 - val_loss: 0.0822 - val_acc: 0.9749\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0804 - acc: 0.9763 - val_loss: 0.0764 - val_acc: 0.9756\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0792 - acc: 0.9763 - val_loss: 0.0765 - val_acc: 0.9757\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0743 - acc: 0.9777 - val_loss: 0.0731 - val_acc: 0.9765\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0720 - acc: 0.9782 - val_loss: 0.0723 - val_acc: 0.9773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7a40ab3160>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Uncertainty Prediction with Monte Carlo Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on test set and compare mean standard deviation of all predictions to mean standard deviation of misclassified samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_with_uncertainty(model, X):\n",
    "    \n",
    "    predictions = []\n",
    "    for i in range(mc_samples): # can be made more efficient by just forward passing several times through the last layer\n",
    "        predictions.append(model.predict(X))\n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    means = np.mean(predictions, axis=0)\n",
    "    std = np.std(predictions, axis=0)\n",
    "    preds = np.argmax(means, axis=1)\n",
    "    preds_std = np.array([std[i, preds[i]] for i in range(len(preds))])\n",
    "\n",
    "    return preds, preds_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9802\n",
      "Average standard deviation of classification: 0.023104193\n",
      "Average standard deviation of misclassified samples: 0.20123567\n"
     ]
    }
   ],
   "source": [
    "preds, stds = get_predictions_with_uncertainty(model, x_test)\n",
    "labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "print('Accuracy: ' + str((preds == labels).sum()/len(labels)))\n",
    "\n",
    "misclassified_mask = labels != preds\n",
    "\n",
    "print('Average standard deviation of classification: ' + str(np.mean(stds)))\n",
    "print('Average standard deviation of misclassified samples: ' + str(np.mean(stds[misclassified_mask])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
