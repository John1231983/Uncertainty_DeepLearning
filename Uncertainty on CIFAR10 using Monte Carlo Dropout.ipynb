{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte-Carlo Dropout for uncertainty prediction on CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement Monte-Carlo dropout for uncertainty prediction according to https://arxiv.org/pdf/1506.02142.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we train a model on CIFAR10 using dropout and then at inference time we keep using dropout to estimate the uncertainty of the network by performing several forward passes through the network for each sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 50\n",
    "mc_samples = 10 # number of samples for prediciton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "img_rows, img_cols = x_train.shape[1], x_train.shape[2]\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 3, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 3, img_rows, img_cols)\n",
    "    input_shape = (3, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 3)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 3)\n",
    "    input_shape = (img_rows, img_cols, 3)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \"\"\"\n",
    "    Use only minimalistic model to get some statistics for misclassifications\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Conv2D(256, kernel_size=(3, 3),\n",
    "#                      activation='relu',\n",
    "#                      input_shape=input_shape))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # This dropout layer stays active during testing phase\n",
    "    model.add(Lambda(lambda x: K.dropout(x, level=0.25)))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    \n",
    "    model.add(Lambda(lambda x: K.dropout(x, level=0.5)))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                optimizer=keras.optimizers.Adadelta(),\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 9s 181us/step - loss: 1.9404 - acc: 0.2856 - val_loss: 1.6198 - val_acc: 0.4037\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 7s 148us/step - loss: 1.4847 - acc: 0.4625 - val_loss: 1.4173 - val_acc: 0.4908\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 1.2910 - acc: 0.5390 - val_loss: 1.3208 - val_acc: 0.5258\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 7s 148us/step - loss: 1.1553 - acc: 0.5930 - val_loss: 1.1045 - val_acc: 0.6087\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 8s 150us/step - loss: 1.0567 - acc: 0.6276 - val_loss: 1.1552 - val_acc: 0.6018\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 7s 150us/step - loss: 0.9805 - acc: 0.6554 - val_loss: 1.0296 - val_acc: 0.6359\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.9209 - acc: 0.6798 - val_loss: 1.0239 - val_acc: 0.6367\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.8636 - acc: 0.6998 - val_loss: 0.9843 - val_acc: 0.6578\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 7s 150us/step - loss: 0.8171 - acc: 0.7175 - val_loss: 1.0170 - val_acc: 0.6487\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.7755 - acc: 0.7300 - val_loss: 0.9790 - val_acc: 0.6658\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.7423 - acc: 0.7395 - val_loss: 0.9583 - val_acc: 0.6714\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 7s 150us/step - loss: 0.7089 - acc: 0.7523 - val_loss: 0.8976 - val_acc: 0.6937\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 7s 150us/step - loss: 0.6795 - acc: 0.7616 - val_loss: 0.9864 - val_acc: 0.6691\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 8s 154us/step - loss: 0.6553 - acc: 0.7701 - val_loss: 0.9448 - val_acc: 0.6844\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 8s 153us/step - loss: 0.6253 - acc: 0.7788 - val_loss: 0.9029 - val_acc: 0.7051\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 8s 150us/step - loss: 0.5995 - acc: 0.7893 - val_loss: 1.1027 - val_acc: 0.6622\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 7s 150us/step - loss: 0.5765 - acc: 0.7960 - val_loss: 0.9577 - val_acc: 0.6975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe6ec24aba8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            validation_data=(x_test, y_test),\n",
    "            callbacks=[EarlyStopping(patience=5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Uncertainty Prediction with Monte Carlo Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on test set and compare mean standard deviation of all predictions to mean standard deviation of misclassified samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_with_uncertainty(model, X):\n",
    "    \n",
    "    predictions = []\n",
    "    for i in range(mc_samples): # can be made more efficient by just forward passing several times through the last layer\n",
    "        predictions.append(model.predict(X))\n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    means = np.mean(predictions, axis=0)\n",
    "    std = np.std(predictions, axis=0)\n",
    "    preds = np.argmax(means, axis=1)\n",
    "    preds_std = np.array([std[i, preds[i]] for i in range(len(preds))])\n",
    "\n",
    "    return preds, preds_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.725\n",
      "Average standard deviation of classification: 0.12922935\n",
      "Average standard deviation of misclassified samples: 0.18558362\n"
     ]
    }
   ],
   "source": [
    "preds, stds = get_predictions_with_uncertainty(model, x_test)\n",
    "labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "print('Accuracy: ' + str((preds == labels).sum()/len(labels)))\n",
    "\n",
    "misclassified_mask = labels != preds\n",
    "\n",
    "print('Average standard deviation of classification: ' + str(np.mean(stds)))\n",
    "print('Average standard deviation of misclassified samples: ' + str(np.mean(stds[misclassified_mask])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
